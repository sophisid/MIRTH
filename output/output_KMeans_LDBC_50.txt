[info] welcome to sbt 1.9.9 (Ubuntu Java 11.0.24)
[info] loading settings for project schemadiscovery-build-build from metals.sbt ...
[info] loading project definition from /mnt/fast/sophisid/cs562_sdpg/schemadiscovery/project/project
[info] loading settings for project schemadiscovery-build from metals.sbt,plugins.sbt ...
[info] loading project definition from /mnt/fast/sophisid/cs562_sdpg/schemadiscovery/project
[success] Generated .bloop/schemadiscovery-build.json
[success] Total time: 3 s, completed Nov 12, 2024, 2:20:55 AM
[info] loading settings for project root from build.sbt ...
[info] set current project to schemadiscovery (in build file:/mnt/fast/sophisid/cs562_sdpg/schemadiscovery/)
[info] running (fork) Main k
[error] Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[error] 24/11/12 02:21:02 INFO SparkContext: Running Spark version 3.4.1
[error] 24/11/12 02:21:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[error] 24/11/12 02:21:02 INFO ResourceUtils: ==============================================================
[error] 24/11/12 02:21:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[error] 24/11/12 02:21:02 INFO ResourceUtils: ==============================================================
[error] 24/11/12 02:21:02 INFO SparkContext: Submitted application: SchemaDiscoveryComparison
[error] 24/11/12 02:21:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[error] 24/11/12 02:21:02 INFO ResourceProfile: Limiting resource is cpu
[error] 24/11/12 02:21:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[error] 24/11/12 02:21:02 INFO SecurityManager: Changing view acls to: sophisid
[error] 24/11/12 02:21:02 INFO SecurityManager: Changing modify acls to: sophisid
[error] 24/11/12 02:21:02 INFO SecurityManager: Changing view acls groups to: 
[error] 24/11/12 02:21:02 INFO SecurityManager: Changing modify acls groups to: 
[error] 24/11/12 02:21:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: sophisid; groups with view permissions: EMPTY; users with modify permissions: sophisid; groups with modify permissions: EMPTY
[error] 24/11/12 02:21:02 INFO Utils: Successfully started service 'sparkDriver' on port 45617.
[error] 24/11/12 02:21:02 INFO SparkEnv: Registering MapOutputTracker
[error] WARNING: An illegal reflective access operation has occurred
[error] WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/mnt/fast/sophisid/cs562_sdpg/schemadiscovery/target/bg-jobs/sbt_78320c0b/target/930ffb2e/b9186082/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[error] WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[error] WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[error] WARNING: All illegal access operations will be denied in a future release
[error] 24/11/12 02:21:02 INFO SparkEnv: Registering BlockManagerMaster
[error] 24/11/12 02:21:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[error] 24/11/12 02:21:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[error] 24/11/12 02:21:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[error] 24/11/12 02:21:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-aa448bb7-e33e-4a6c-94c7-661696f8ec45
[error] 24/11/12 02:21:02 INFO MemoryStore: MemoryStore started with capacity 17.8 GiB
[error] 24/11/12 02:21:02 INFO SparkEnv: Registering OutputCommitCoordinator
[error] 24/11/12 02:21:02 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[error] 24/11/12 02:21:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[error] 24/11/12 02:21:03 INFO Executor: Starting executor ID driver on host clusternode4
[error] 24/11/12 02:21:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[error] 24/11/12 02:21:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33731.
[error] 24/11/12 02:21:03 INFO NettyBlockTransferService: Server created on clusternode4:33731
[error] 24/11/12 02:21:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[error] 24/11/12 02:21:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, clusternode4, 33731, None)
[error] 24/11/12 02:21:03 INFO BlockManagerMasterEndpoint: Registering block manager clusternode4:33731 with 17.8 GiB RAM, BlockManagerId(driver, clusternode4, 33731, None)
[error] 24/11/12 02:21:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, clusternode4, 33731, None)
[error] 24/11/12 02:21:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, clusternode4, 33731, None)
[info] Loading all nodes from Neo4j
[info] Total nodes loaded: 3181724
[info] Schema of nodesDF:
[info] root
[info]  |-- _labels: string (nullable = true)
[info]  |-- browserUsed: string (nullable = true)
[info]  |-- lastName: string (nullable = true)
[info]  |-- content: string (nullable = true)
[info]  |-- _nodeId: string (nullable = true)
[info]  |-- type: string (nullable = true)
[info]  |-- name: string (nullable = true)
[info]  |-- firstName: string (nullable = true)
[info]  |-- gender: string (nullable = true)
[info]  |-- id: string (nullable = true)
[info]  |-- locationIP: string (nullable = true)
[info]  |-- title: string (nullable = true)
[info]  |-- imageFile: string (nullable = true)
[info]  |-- url: string (nullable = true)
[info]  |-- length: string (nullable = true)
[info]  |-- language: string (nullable = true)
[info]  |-- creationDate: string (nullable = true)
[info]  |-- birthday: string (nullable = true)
[info] Loading all relationships from Neo4j
[info] Time taken for Data Loading and Preprocessing: 57.20 seconds
[info] Features assembled for k-means clustering:
[info] +-------+--------------------+
[info] |_nodeId|            features|
[info] +-------+--------------------+
[info] |      0|(17,[0,1,3,8,9,13...|
[info] |      1|(17,[0,1,3,8,9,13...|
[info] |      2|(17,[0,1,3,8,9,13...|
[info] |      3|(17,[0,1,3,8,9,13...|
[info] |      4|(17,[0,1,3,8,9,13...|
[info] +-------+--------------------+
[info] only showing top 5 rows
[info] Number of distinct types/patterns in Neo4j (including 'NoLabel'): 8
[info] Setting k for K-Means clustering to: 8
[error] 24/11/12 02:24:06 ERROR Instrumentation: java.lang.ClassNotFoundException: scala.math.Ordering$Reverse
[error] 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
[error] 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
[error] 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
[error] 	at java.base/java.lang.Class.forName0(Native Method)
[error] 	at java.base/java.lang.Class.forName(Class.java:398)
[error] 	at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
[error] 	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:224)
[error] 	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:104)
[error] 	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
[error] 	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:111)
[error] 	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:351)
[error] 	at org.apache.spark.serializer.KryoSerializerInstance.getAutoReset(KryoSerializer.scala:451)
[error] 	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects$lzycompute(KryoSerializer.scala:258)
[error] 	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects(KryoSerializer.scala:254)
[error] 	at org.apache.spark.shuffle.sort.SortShuffleManager$.canUseSerializedShuffle(SortShuffleManager.scala:233)
[error] 	at org.apache.spark.shuffle.sort.SortShuffleManager.registerShuffle(SortShuffleManager.scala:106)
[error] 	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:103)
[error] 	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:255)
[error] 	at scala.Option.getOrElse(Option.scala:189)
[error] 	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:251)
[error] 	at org.apache.spark.scheduler.DAGScheduler.visit$2(DAGScheduler.scala:788)
[error] 	at org.apache.spark.scheduler.DAGScheduler.eagerlyComputePartitionsForRddAndAncestors(DAGScheduler.scala:795)
[error] 	at org.apache.spark.scheduler.DAGScheduler.submitJob(DAGScheduler.scala:913)
[error] 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:961)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1018)
[error] 	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$countByKey$1(PairRDDFunctions.scala:367)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:367)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$countByValue$1(RDD.scala:1312)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1312)
[error] 	at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:448)
[error] 	at org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:273)
[error] 	at org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)
[error] 	at org.apache.spark.ml.clustering.KMeans.trainWithRow(KMeans.scala:446)
[error] 	at org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:382)
[error] 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
[error] 	at scala.util.Try$.apply(Try.scala:213)
[error] 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
[error] 	at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:371)
[error] 	at Clustering$.performKMeansClustering(Clustering.scala:241)
[error] 	at Main$.runKMeansClustering(Main.scala:157)
[error] 	at Main$.main(Main.scala:63)
[error] 	at Main.main(Main.scala)
[error] Exception in thread "main" java.lang.ClassNotFoundException: scala.math.Ordering$Reverse
[error] 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
[error] 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
[error] 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
[error] 	at java.base/java.lang.Class.forName0(Native Method)
[error] 	at java.base/java.lang.Class.forName(Class.java:398)
[error] 	at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
[error] 	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:224)
[error] 	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:104)
[error] 	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
[error] 	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:111)
[error] 	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:351)
[error] 	at org.apache.spark.serializer.KryoSerializerInstance.getAutoReset(KryoSerializer.scala:451)
[error] 	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects$lzycompute(KryoSerializer.scala:258)
[error] 	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects(KryoSerializer.scala:254)
[error] 	at org.apache.spark.shuffle.sort.SortShuffleManager$.canUseSerializedShuffle(SortShuffleManager.scala:233)
[error] 	at org.apache.spark.shuffle.sort.SortShuffleManager.registerShuffle(SortShuffleManager.scala:106)
[error] 	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:103)
[error] 	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:255)
[error] 	at scala.Option.getOrElse(Option.scala:189)
[error] 	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:251)
[error] 	at org.apache.spark.scheduler.DAGScheduler.visit$2(DAGScheduler.scala:788)
[error] 	at org.apache.spark.scheduler.DAGScheduler.eagerlyComputePartitionsForRddAndAncestors(DAGScheduler.scala:795)
[error] 	at org.apache.spark.scheduler.DAGScheduler.submitJob(DAGScheduler.scala:913)
[error] 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:961)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1018)
[error] 	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$countByKey$1(PairRDDFunctions.scala:367)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:367)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$countByValue$1(RDD.scala:1312)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1312)
[error] 	at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:448)
[error] 	at org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:273)
[error] 	at org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)
[error] 	at org.apache.spark.ml.clustering.KMeans.trainWithRow(KMeans.scala:446)
[error] 	at org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:382)
[error] 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
[error] 	at scala.util.Try$.apply(Try.scala:213)
[error] 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
[error] 	at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:371)
[error] 	at Clustering$.performKMeansClustering(Clustering.scala:241)
[error] 	at Main$.runKMeansClustering(Main.scala:157)
[error] 	at Main$.main(Main.scala:63)
[error] 	at Main.main(Main.scala)
[error] Nonzero exit code returned from runner: 1
[error] (Compile / run) Nonzero exit code returned from runner: 1
[error] Total time: 192 s (03:12), completed Nov 12, 2024, 2:24:08 AM
