[info] welcome to sbt 1.9.9 (Ubuntu Java 11.0.25)
[info] loading settings for project schemadiscovery-build-build from metals.sbt ...
[info] loading project definition from /mnt/fast/sophisid/cs562_sdpg/schemadiscovery/project/project
[info] loading settings for project schemadiscovery-build from metals.sbt,plugins.sbt ...
[info] loading project definition from /mnt/fast/sophisid/cs562_sdpg/schemadiscovery/project
[success] Generated .bloop/schemadiscovery-build.json
[success] Total time: 3 s, completed Nov 14, 2024, 3:32:16 PM
[info] loading settings for project root from build.sbt ...
[info] set current project to schemadiscovery (in build file:/mnt/fast/sophisid/cs562_sdpg/schemadiscovery/)
[info] running (fork) Main k
[error] Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[error] 24/11/14 15:32:22 INFO SparkContext: Running Spark version 3.4.1
[error] 24/11/14 15:32:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[error] 24/11/14 15:32:22 INFO ResourceUtils: ==============================================================
[error] 24/11/14 15:32:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[error] 24/11/14 15:32:22 INFO ResourceUtils: ==============================================================
[error] 24/11/14 15:32:22 INFO SparkContext: Submitted application: SchemaDiscoveryComparison
[error] 24/11/14 15:32:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[error] 24/11/14 15:32:22 INFO ResourceProfile: Limiting resource is cpu
[error] 24/11/14 15:32:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[error] 24/11/14 15:32:22 INFO SecurityManager: Changing view acls to: sophisid
[error] 24/11/14 15:32:22 INFO SecurityManager: Changing modify acls to: sophisid
[error] 24/11/14 15:32:22 INFO SecurityManager: Changing view acls groups to: 
[error] 24/11/14 15:32:22 INFO SecurityManager: Changing modify acls groups to: 
[error] 24/11/14 15:32:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: sophisid; groups with view permissions: EMPTY; users with modify permissions: sophisid; groups with modify permissions: EMPTY
[error] 24/11/14 15:32:22 INFO Utils: Successfully started service 'sparkDriver' on port 40839.
[error] 24/11/14 15:32:22 INFO SparkEnv: Registering MapOutputTracker
[error] WARNING: An illegal reflective access operation has occurred
[error] WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/mnt/fast/sophisid/cs562_sdpg/schemadiscovery/target/bg-jobs/sbt_51affe9a/target/930ffb2e/b9186082/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[error] WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[error] WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[error] WARNING: All illegal access operations will be denied in a future release
[error] 24/11/14 15:32:23 INFO SparkEnv: Registering BlockManagerMaster
[error] 24/11/14 15:32:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[error] 24/11/14 15:32:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[error] 24/11/14 15:32:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[error] 24/11/14 15:32:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-55d69241-14da-4f36-92cd-b7d49b5a70d8
[error] 24/11/14 15:32:23 INFO MemoryStore: MemoryStore started with capacity 17.8 GiB
[error] 24/11/14 15:32:23 INFO SparkEnv: Registering OutputCommitCoordinator
[error] 24/11/14 15:32:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[error] 24/11/14 15:32:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[error] 24/11/14 15:32:23 INFO Executor: Starting executor ID driver on host clusternode4
[error] 24/11/14 15:32:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[error] 24/11/14 15:32:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35231.
[error] 24/11/14 15:32:23 INFO NettyBlockTransferService: Server created on clusternode4:35231
[error] 24/11/14 15:32:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[error] 24/11/14 15:32:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, clusternode4, 35231, None)
[error] 24/11/14 15:32:23 INFO BlockManagerMasterEndpoint: Registering block manager clusternode4:35231 with 17.8 GiB RAM, BlockManagerId(driver, clusternode4, 35231, None)
[error] 24/11/14 15:32:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, clusternode4, 35231, None)
[error] 24/11/14 15:32:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, clusternode4, 35231, None)
[info] Loading all nodes from Neo4j
[info] Total nodes loaded: 486267
[info] Schema of nodesDF:
[info] root
[info]  |-- post: string (nullable = true)
[info]  |-- _labels: string (nullable = true)
[info]  |-- cropped: string (nullable = true)
[info]  |-- postHighAccuracyThreshold: string (nullable = true)
[info]  |-- alpha3: string (nullable = true)
[info]  |-- totalPreCount: string (nullable = true)
[info]  |-- _nodeId: string (nullable = true)
[info]  |-- type: string (nullable = true)
[info]  |-- latestMutationId: string (nullable = true)
[info]  |-- instance: string (nullable = true)
[info]  |-- bodyId: string (nullable = true)
[info]  |-- statusLabel: string (nullable = true)
[info]  |-- primaryRois: string (nullable = true)
[info]  |-- neuroglancerInfo: string (nullable = true)
[info]  |-- statusDefinitions: string (nullable = true)
[info]  |-- size: string (nullable = true)
[info]  |-- alpha2: string (nullable = true)
[info]  |-- location: string (nullable = true)
[info]  |-- alpha1: string (nullable = true)
[info]  |-- totalPostCount: string (nullable = true)
[info]  |-- roiHierarchy: string (nullable = true)
[info]  |-- postHPThreshold: string (nullable = true)
[info]  |-- dataset: string (nullable = true)
[info]  |-- roiInfo: string (nullable = true)
[info]  |-- superLevelRois: string (nullable = true)
[info]  |-- status: string (nullable = true)
[info]  |-- confidence: string (nullable = true)
[info]  |-- lastDatabaseEdit: string (nullable = true)
[info]  |-- neuroglancerMeta: string (nullable = true)
[info]  |-- uuid: string (nullable = true)
[info]  |-- meshHost: string (nullable = true)
[info]  |-- pre: string (nullable = true)
[info] Loading all relationships from Neo4j
[info] Time taken for Data Loading and Preprocessing: 13.45 seconds
[info] Features assembled for k-means clustering:
[info] +-------+--------------+
[info] |_nodeId|      features|
[info] +-------+--------------+
[info] |      0|(31,[1],[1.0])|
[info] |      1|(31,[1],[1.0])|
[info] |      2|(31,[1],[1.0])|
[info] |      3|(31,[1],[1.0])|
[info] |      4|(31,[1],[1.0])|
[info] +-------+--------------+
[info] only showing top 5 rows
[info] Number of distinct types/patterns in Neo4j (including 'NoLabel'): 6
[info] Setting k for K-Means clustering to: 6
[error] 24/11/14 15:32:57 ERROR Instrumentation: java.lang.ClassNotFoundException: scala.math.Ordering$Reverse
[error] 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
[error] 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
[error] 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
[error] 	at java.base/java.lang.Class.forName0(Native Method)
[error] 	at java.base/java.lang.Class.forName(Class.java:398)
[error] 	at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
[error] 	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:224)
[error] 	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:104)
[error] 	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
[error] 	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:111)
[error] 	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:351)
[error] 	at org.apache.spark.serializer.KryoSerializerInstance.getAutoReset(KryoSerializer.scala:451)
[error] 	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects$lzycompute(KryoSerializer.scala:258)
[error] 	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects(KryoSerializer.scala:254)
[error] 	at org.apache.spark.shuffle.sort.SortShuffleManager$.canUseSerializedShuffle(SortShuffleManager.scala:233)
[error] 	at org.apache.spark.shuffle.sort.SortShuffleManager.registerShuffle(SortShuffleManager.scala:106)
[error] 	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:103)
[error] 	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:255)
[error] 	at scala.Option.getOrElse(Option.scala:189)
[error] 	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:251)
[error] 	at org.apache.spark.scheduler.DAGScheduler.visit$2(DAGScheduler.scala:788)
[error] 	at org.apache.spark.scheduler.DAGScheduler.eagerlyComputePartitionsForRddAndAncestors(DAGScheduler.scala:795)
[error] 	at org.apache.spark.scheduler.DAGScheduler.submitJob(DAGScheduler.scala:913)
[error] 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:961)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1018)
[error] 	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$countByKey$1(PairRDDFunctions.scala:367)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:367)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$countByValue$1(RDD.scala:1312)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1312)
[error] 	at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:448)
[error] 	at org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:273)
[error] 	at org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)
[error] 	at org.apache.spark.ml.clustering.KMeans.trainWithRow(KMeans.scala:446)
[error] 	at org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:382)
[error] 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
[error] 	at scala.util.Try$.apply(Try.scala:213)
[error] 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
[error] 	at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:371)
[error] 	at Clustering$.performKMeansClustering(Clustering.scala:241)
[error] 	at Main$.runKMeansClustering(Main.scala:175)
[error] 	at Main$.main(Main.scala:63)
[error] 	at Main.main(Main.scala)
[error] Exception in thread "main" java.lang.ClassNotFoundException: scala.math.Ordering$Reverse
[error] 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
[error] 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
[error] 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
[error] 	at java.base/java.lang.Class.forName0(Native Method)
[error] 	at java.base/java.lang.Class.forName(Class.java:398)
[error] 	at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
[error] 	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:224)
[error] 	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:104)
[error] 	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
[error] 	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:111)
[error] 	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:351)
[error] 	at org.apache.spark.serializer.KryoSerializerInstance.getAutoReset(KryoSerializer.scala:451)
[error] 	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects$lzycompute(KryoSerializer.scala:258)
[error] 	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects(KryoSerializer.scala:254)
[error] 	at org.apache.spark.shuffle.sort.SortShuffleManager$.canUseSerializedShuffle(SortShuffleManager.scala:233)
[error] 	at org.apache.spark.shuffle.sort.SortShuffleManager.registerShuffle(SortShuffleManager.scala:106)
[error] 	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:103)
[error] 	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:255)
[error] 	at scala.Option.getOrElse(Option.scala:189)
[error] 	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:251)
[error] 	at org.apache.spark.scheduler.DAGScheduler.visit$2(DAGScheduler.scala:788)
[error] 	at org.apache.spark.scheduler.DAGScheduler.eagerlyComputePartitionsForRddAndAncestors(DAGScheduler.scala:795)
[error] 	at org.apache.spark.scheduler.DAGScheduler.submitJob(DAGScheduler.scala:913)
[error] 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:961)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
[error] 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1018)
[error] 	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$countByKey$1(PairRDDFunctions.scala:367)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:367)
[error] 	at org.apache.spark.rdd.RDD.$anonfun$countByValue$1(RDD.scala:1312)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[error] 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
[error] 	at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1312)
[error] 	at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:448)
[error] 	at org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:273)
[error] 	at org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)
[error] 	at org.apache.spark.ml.clustering.KMeans.trainWithRow(KMeans.scala:446)
[error] 	at org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:382)
[error] 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
[error] 	at scala.util.Try$.apply(Try.scala:213)
[error] 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
[error] 	at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:371)
[error] 	at Clustering$.performKMeansClustering(Clustering.scala:241)
[error] 	at Main$.runKMeansClustering(Main.scala:175)
[error] 	at Main$.main(Main.scala:63)
[error] 	at Main.main(Main.scala)
[error] Nonzero exit code returned from runner: 1
[error] (Compile / run) Nonzero exit code returned from runner: 1
[error] Total time: 41 s, completed Nov 14, 2024, 3:32:57 PM
